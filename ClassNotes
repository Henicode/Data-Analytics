
                WEEK 1

Data Engineer - knowledge of programming, sound knowledge of systems and technology architectures, in-depth understanding of rational databases and 
                non-rantional data stores.
Data Analyst - Inspect and clean data for derving insights, identify correlations, find patterns, and apply statistical methods to analyze and mine data,
                visualize data to interpret and present the findings of data analysis. 
                
                
Types of analysis -
                    Descriptive Analytics which provides insights into past events. “What happened.” 
                    Diagnostic Analytics takes the insights from descrptive analytics to dig deeper to find the cause of the outcome. “Why it happened.” 
                    Predictive Analytics leverages historical data and trends to predict future outcomes. “What will happen next.” 
                    Prescriptive Analytics analyzes past decisions and events to estimate the likelihood of different outcomes. “What should be done next.” 
                    
Analysis Process - 
       .Understanding the problem and desired result - defining where you are and where you want to be
       .Setting a clear metric - deciding what will be measured and how it will be measured 
       .Gathering data - identifying data you require, the sources from which you will access this data, and the best tool for the job
       .Cleaning data - fixing quality issues in the data and standardizing data coming in from multiple sources
       .Analyzing and mining data - extracting, analyzing, and manipulating data from different perespectives to understand trends identify
                                     correlations, and find patterns and variations
       .Interpreting results - interpreting results, evaluating defendability of analysis and circumastances under which analysis may not hold true
       .Presenting your findings - communicating and presenting your findings in clear, impactful, and convincing ways 

Cloud computing is the emerging technology that has made it possible for every enterprise to have access to limitless storage and high-performance computing.
                With cloud computing, businesses can store, manage, and process large amounts of data in a remote, centralized location.  
                
                
Analysis - detailed examination of the elements or structure of something

Analytics - the systematic computational analysis of data or statistics


     
                                        ------------
                                         
  Technical skills - 
                 .Spreadsheets such as Excel and Google Sheets
                 .Proficiency in statistical analysis and visualization tools and software - IBM Cognos, IBM SPSS, Oracle Visual analyzer,
                                                                                              Power BI, SAS, Tableau
                 .Programming languages - R, Python, C++, Java, MATLAB
                 .SQL and ability to work with data in relational and NoSQL databases 
                 .Data Repositories - Data marts, data warehouses, data lakes, and data pipelines
                 .Big data processing tools - Hadoop, Hive, and Spark 
   
  Functional skills -
                 .Proficiency in statics - analyze data, validate the analysis, identify fallacies and logical errors
                 .Analytical skills - researcha nd interpret data, theorize, make forecasts 
                 .Problem-solving skills - come up with possible solutions for a given problem
                 .Probing skills - identify and define the proble statement and desired outcome 
                 .Data visualization skills - create clear and compelling visualizations to present the analysis 
                 .Project management skills - manage the process, people, dependencies, and timeline 
                 
 Soft skills -
                 .work collaboratively with business and cross-function teams
                 .communicate effectively to report and present your findings
                 .tell a compelling convicing story
                 .gather support and buy in for your work 
                 .Curiosity - allowing new questions to surface and challenging your own assumptions and hypotheses
                 .Intuition - having sense of the future based on pattern recognition and past experiences 
                 
          
          
      WEEK 2 
                          -- The Data Ecosystem --
          
          
      A data analyst's ecosystem includes the infrastructure, software, tools, frameworks, and processes used to gather, clean, mine, visualize data.
      
      Types of data - 
                     .Structured data - follows a rigid format and can be organized into rows and columns. - databses, spreadsheets
                                      - is data that is well organized in formats that can be stored in databses and lends itself to standard data 
                                         analysis methods and tools
                     .Semi-structured - mix of data that has consistent characteristics and data that does not corform to a rigid structure. - email
                                      - is data that is somewhat organized and relies on meta tags for grouping and hierachy
                     .Unstructured - data that is complex and mostly qualitative information that cannot be structured into rows and columns.  - photo, text files
                                   - is data that is not conventionally organized in the form of rows and columns in a particular format.
                                    
                                    
      File formats of data - Relational databse, Non-relational database, APIs, Web services, Data streams, Social Platforms, Sensor Devices. 
      
      Languages available in data ecosystem : Query (querying and manipulating data), Programming (developing data application),
                                              Shell and Scripting Languages (repetitive operational tasks).
                                              
                                              
                                              
                               -- Types of Data -- 
                               
           Data comprises of facts, observation, preceptions, numbers, characters, symbols, images that can be interpreted to derive meaning. 
           
An API, or application programming interface, is a set of rules and protocols that allows different software systems to communicate with each other.
 Essentially, an API specifies how software components should interact, and allows developers to access the functionality of another system or service.

Introduction in Data Languages - Query Languages, Programming Languages, Shell Scripting. 
           
 Shell and scripting languages are commonly used for automating repetitive operational tasks.
 API can return data in awide variety of formats such as plain text, XML, HTML, or JSON among others. 
 Python supports multiple programming paradigms, such as object-orientedm imperative, functional, and procedural, making it suitable for a wide variety of use cases.
 
 
 Databse is the collection of data for input, storage, search, retrieval, and modification of data. 
 DBMB database management system is a set of programs for creating and maintaining the databse, and storing, modifying, and extracting information
                                 from the database using a quering function.
                                 
 Relational database - 
                      .Data is organized into a tabular format with rows and columns 
                      .Well-defined structure and shcema 
                      .Optimized for data operations and querying 
                      .Use SQL as the standard querying language
Non-Relational Databases - 
                      .Emerged in response to the volume, diversity, and speed at which data is being generated today
                      .Built for speed, flexability, and scale
                      .Data can be stored in a schema-less form
                      .Widely used for processing big data 
Key Differences 
                Relational databases - 
                                      .RDBMS schemas rigidly define how all data inserted into the database must be typed and composed
                                      .Maintaining high-end, commercial relational database management systems can be expensive
                                      .Support ACID-compliance, which ensures reliablility of transactions and crash recovery
                                      .A mature and well-documented technology, which means the risks are more or less perceivable
                Non-Relationa database - 
                                      .NoSQL databases can be schema-agnostic, allowing unstructured and semi-structured data to be stored and manipulated
                                      .Specifically designed for low-cost commodity hardware
                                      .Most NoSQL databases are not ACID compliant 
                                      .A relatively newer technology 
                                      
                                      
Data warehouse works as a central repository that merges information coming from disparete sources and consolidates it through the extract,
               transform, and load process, also known as ETL process, into one comprehensive database for analytics and business intelligence. 
               
 Data repositories help isolate data and make reporting and analytics more efficient and credible while also serving as a data archive.   
 
  The different types of Data Repositories include: 
            .Databases, which can be relational or non-relational, each following a set of organizational principles, the types of data they can store,
             and the tools that can be used to query, organize, and retrieve data.
            .Data Warehouses, that consolidate incoming data into one comprehensive storehouse.  
            .Data Marts, that are essentially sub-sections of a data warehouse, built to isolate data for a particular business function or use case. 
            .Data Lakes, that serve as storage repositories for large amounts of structured, semi-structured, and unstructured data in their
             native format. 
            .Big Data Stores, that provide distributed computational and storage infrastructure to store, scale, and process very large data sets.                    

  Document-based NoSQL database store each record and its associated data within a single document and work well with Analytics plateforms.
  
 
 
 WEEK 3 
          -- Gathering Data --
          
     Process for identifying data -
                                  1. Determine the information you want to collect 
                                     .The specific information you need 
                                     .The possible sources for the data
                                  2. Define a plan for collecting data
                                     .Establish a timeframe for collecting data
                                     .How much data is sufficient for a credible analysis
                                     .Define dependencies, risks, and mitigation plan
                                  3. Determine your data collection methods. Methods depend on:
                                     .Sources of data
                                     .Type of data
                                     .Timeframe over which you need the data
                                     .Volume of data 
    Key considerations :
         The data you identify, the source of that data, and the practices you employ for gathering the fata have implications for :
          1. Quality 
              data needs to be free of errors, accurate, complete, relevant, accessible
          2. Security
              policies and procedures related to the usablility, integrity, and availability of dat. issus include security, regulation, compliances.
          3. Privacy 
              need to define checks, validations, and auditable trail. privacy includes confidentiality, compliance to mandate regulations, lices for use
          
          
             -- Data Sources --
             
       Data sources can be internal or external to the organization. And can come from a Primary, Secondary, or Third-party sources. 
       
       Primary data refers to the information obtained directly from the sources. 
                   it is Data that has been generated by the researcher, surveys, interviews, experiments, specially designed for understanding and
                    solving the research problem at hand.       
       Secondary data referes to information retrieved from exisiting sources. 
                   it is an external databses. research articles, publications, internet searches. 
       Third-party data refers to data purchased from aggregators who collect data from various sources and combine it into comprehensive 
                   datasets for purpose of selling the data. Online campaigns, research companies, social media plugins, publishers. 
                   
            -- Data Wrangline --
            
    Data wrangline, also known as data munging, is an iterative process that involes data exploration, transformation, validation, 
                    and making it available for a credible and meaningful analysis.
                    
     Wrangling exploration phase:
                        1. Discovery : examining and understanding your data 
                                       creating a plan for cleaning, structuring, organizing, and mapping your data.
                        2. Transformation : Structured - actions that change the form and schema of your data. JOINS(comb columns) and UNIONS(comb rows) 
                                            Normalizing data - cleaning unused data, reducing redundancy, reducing incosistency 
                                            denormalizing data - combining data from multiple tables into a single table for fast querying 
                                            Cleaning data - fixing irregularities in data in order to produce a credible and accurate analysis 
                                            Enriching data - adding data points that make your analysis more meaningful. combinign tables for more info
                       3. Validation : checking the quality of data after structuring, normalizing, denormalizing, cleaning, and enriching of data
                                       verifying consistency, quality, and security of data. 
                       4. Publishing : delivering the output of the wrangled data for downstream project needs. 
                       
                     
A variety of software and tools are available for the Data Wrangling process. Some of the popularly used ones include Excel Power Query,
Spreadsheets, OpenRefine, Google DataPrep, Watson Studio Refinery, Trifacta Wrangler, Python, and R, each with their own set of characteristics,
strengths, limitations, and applications. 
    
    Data cleaning is a subset of data wrangling which falls under the transformation step.
              some issuse are: Missing values, duplicates, irrelevant, syntex error, outliers
                              
   
   
   WEEK 4
            -- analyzing and mining data --
            
     Statistics is a branch of mathematics dealing with the collection, analysis, interpretation, and presentation of numerical or quantitative data.
     
     Statistical analysis is the application of statistical methods to a sample of data in order to develop an understanding of what that data represents.
                 Sample - is a representative selection drawn from a total population.
                 Population - is a discrete group of people or things that can be identified by at least one common characteristic for purposes of data
                              collection and analysis.
                              
    Type of statistics : 
                        .Descriptive statistics - summarizing information about the sample. 
                                                - objective is to make it easier to understand and visualize data without making conclusions regarding
                                                  any hypotheses that were made.
                                                - Measures of Descriptive stats are :
                                                           1. Central tendency - locating the center of a data sample. EX. Mean(avg), Median(middle set), and Mode(most common).
                                                           2. Dispersion - the mesure of variablility in a dataset. 
                                                                           EX. Variance - how far away the data point fall from the center
                                                                               Standard Deviation - how tightly your data is clustered aroudn the mean
                                                                               Range - the distance between the samllest and larges values in datasets.
                                                           3. Skewness - the measure of whether the distribution of values is symmetrical around a central 
                                                                         value or skewed left or right.
                        .Inferential statistics - making inference or generalizaiton about the broad population 
                                                - Methods of IS :
                                                                 .Hypothesis testing
                                                                 .Confidence intervals
                                                                 .Regression analysis 
                                                                 
                                                             
 Data Mining :
              The process of extracting knowledge from data
              An interdisciplinary field that involves the use of pattern recognition technologies, statistical analysis, and mathematical techaniques
              Aims to identify correlations in data, find patterns and variations, understand trends, and predics probabilities 
              
  
  Data visualization is the communication of information through visual elements like graphs, charts ext to make it easy to comprehend, interept,
                     and retain.
                     
Data has value through the stories that it tells. In order to communicate your findings impactfully, you need to: 
            Ensure that your audience is able to trust you, understand you, and relate to your findings and insights.
            Establish the credibility of your findings.
            Present the data within a structured narrative.
            Support your communication with strong visualizations so that the message is clear and concise, and drives your audience to take action.

                     
                     
                     
  Commonly used visualization satware and tools include:
      .Spreadsheets - commonly used software for gpahical representation of data sets
      .Jupyter notbook - an open-source web application tha provides a way to explore data and create visualizations.
      .Python librares - libraries like Matplotlib help create high-quality interactive graphs and plots with few codes. different kinds of 2D AND 3D plots
                       - Bokeh provides interactive charts and plots. delivers high-performace interactivity over large datasets. can transform visuals writeen in other python libraries
                       - Dash is a python framework to create interacive web-based visualizations. it easily maintainable, cross-platform, mobile-ready.
      .R-studio - helps creat basic(bar charts, line chart,ext) and advanced visualizations(head maps, mosaic maps, 3D graphs). 
      .R-shiny - its an R package that helps build interactive web apps that can be hosted as standalone apps on webpages. easy to use
      .IBM cognos analytics - an end-to-end analytics solution. can import custom visualizations, recommend visuals based on data.
      .Tableau - software company that produces interactive data visualization products. can publish results in the form of stories, also has the option to import R and Python scripts
      .Power BI - cloud-based analytics service from microsoft that enables you to create reports and dashboards. its a powerful and flexible with high speed and efficiency. compatible with excel, sql, cloud-based reporsitories
   
          
          
          
          
          
                                         
                                         
                                         
                                         
                                         
                                         
                                         
                                         
                                         
                                         
                                         
                                         
                                         
                                         
                                         
                                         
                                         
                                         
                                         
                                         
                                         
                                         
                                         
                                         
                                         
                                         
                                         
                                         
